import tensorflow as tf


a=tf.constant(15)
b=tf.constant(20)
print(a*b)

x=np.random.rand(100).astype(np.float32)
print(x)
y=0.2*x+0.2
W=tf.Variable(tf.random.normal([1]))
b=tf.Variable(tf.zeros([1]))
def mse_loss():
    y_pred=W*x+b
    loss=tf.reduce_mean(tf.square(y_pred-y))
    return loss
optimizer=tf.optimizers.Adam()
for step in range(5000):
    optimizer.minimize(mse_loss, var_list=[W,b])
    if step%500==0:
        print(step, W.numpy(),b.numpy())


model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(32, )),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

import numpy as np
x_train = np.random.rand(1000, 32)
y_train = np.random.randint(0, 10, size=(1000,))

model.fit(x_train, y_train, epochs=10)

tensor_from_list = tf.constant([[1, 2], [3, 4]])
print(tensor_from_list)

random_tensor = tf.random.uniform(shape=(2, 4), minval = 32, maxval = 64)
print(random_tensor)

zero_tensor = tf.zeros(shape=(2, 3))
print(zero_tensor)

one_tensor = tf.ones(shape=(2, 3))
print(one_tensor)

a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 6], [7, 8]])

print(tf.add(a, b))
print(tf.subtract(a, b))
print(tf.multiply(a, b))
print(tf.divide(a, b))

np_arr = np.array([[1, 2], [3, 4]])
tensor_from_np = tf.convert_to_tensor(np_arr)
print(tensor_from_np)

# 4. PyTorch

import torch
import torch.nn as nn
import torch.optim as optim
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(32, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())
x_train = torch.rand(1000, 32)
y_train = torch.randint(0, 10, (1000,))

for epoch in range(10):
    model.train()
    optimizer.zero_grad()
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')

data = [[1, 2], [3, 4]]
tensor_from_list = torch.tensor(data)
print("Tensor from list:\n", tensor_from_list)

random_tensor = torch.rand((2, 3))
print("Random tensor:\n", random_tensor)

zero_tensor = torch.zeros((2, 3))
print("Zero tensor:\n", zero_tensor)

ones_tensor = torch.ones((2, 3))
print("Ones tensor:\n", ones_tensor)

a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)
b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)
print("Addition:\n", a + b)
print("Subtraction:\n", a - b)
print("Multiplication:\n", a * b)
print("Division:\n", a / b)

sqrt_tensor = torch.sqrt(a)
print("Square root tensor:\n", sqrt_tensor)

exp_tensor = torch.exp(a)
print("Exponential tensor:\n", exp_tensor)

log_tensor = torch.log(a)
print("Logarithm tensor:\n", log_tensor)

tensor_np = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
np_array = tensor_np.numpy()  # Convert to NumPy array
print("Converted to NumPy array:\n", np_array)

np_array = np.array([[5.0, 6.0], [7.0, 8.0]])
tensor_from_np = torch.from_numpy(np_array)  # Convert NumPy array to tensor
print("Converted back to Tensor:\n", tensor_from_np)


















